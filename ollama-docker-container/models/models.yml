# Ollama Model Containerizer - Models Configuration
# Author: aoton0029
# Date: 2025-08-19

models:
  # Llama 2 Models
  llama2:
    file: "models/llama2/model.bin"
    config: "llama2-7b-chat"
    port: 11434
    description: "Meta's Llama 2 7B chat model"
    requirements:
      memory: "8GB"
      storage: "4GB"
  
  llama2-13b:
    file: "models/llama2-13b/model.bin"
    config: "llama2-13b-chat"
    port: 11435
    description: "Meta's Llama 2 13B chat model"
    requirements:
      memory: "16GB"
      storage: "8GB"

  # Code Llama Models
  codellama:
    file: "models/codellama/model.bin"
    config: "codellama-7b-instruct"
    port: 11436
    description: "Meta's Code Llama 7B instruct model"
    requirements:
      memory: "8GB"
      storage: "4GB"

  codellama-python:
    file: "models/codellama-python/model.bin"
    config: "codellama-7b-python"
    port: 11437
    description: "Code Llama 7B specialized for Python"
    requirements:
      memory: "8GB"
      storage: "4GB"

  # Mistral Models
  mistral:
    file: "models/mistral/model.bin"
    config: "mistral-7b-instruct"
    port: 11438
    description: "Mistral AI's 7B instruct model"
    requirements:
      memory: "8GB"
      storage: "4GB"

  # Neural Chat
  neural-chat:
    file: "models/neural-chat/model.bin"
    config: "neural-chat-7b"
    port: 11439
    description: "Intel's Neural Chat 7B model"
    requirements:
      memory: "8GB"
      storage: "4GB"

  # Zephyr
  zephyr:
    file: "models/zephyr/model.bin"
    config: "zephyr-7b-beta"
    port: 11440
    description: "HuggingFace's Zephyr 7B Beta"
    requirements:
      memory: "8GB"
      storage: "4GB"

# デフォルト設定
defaults:
  memory_limit: "8GB"
  cpu_limit: 4
  timeout: 300
  max_concurrent: 1

# 共通設定
common:
  base_image: "ollama-base:latest"
  health_check_interval: 30
  restart_policy: "unless-stopped"
  log_driver: "json-file"
  log_options:
    max-size: "100m"
    max-file: "3"