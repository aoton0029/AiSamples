import asyncio
import logging
import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from enhanced_document_service import EnhancedDocumentService

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def main():
    """æ‹¡å¼µæ©Ÿèƒ½ãƒ‡ãƒ¢"""
    service = None
    
    try:
        logger.info("=== Enhanced LlamaIndex Multi-Database Architecture Demo ===")
        logger.info("Initializing enhanced document service...")
        
        service = EnhancedDocumentService()
        success = await service.initialize()
        
        if not success:
            logger.error("Failed to initialize enhanced document service")
            return
        
        logger.info("âœ… Enhanced document service initialized successfully")
        
        # æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆè¡¨ç¤º
        await display_enhanced_stats(service)
        
        # æ‹¡å¼µã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
        await create_enhanced_sample_documents(service)
        
        # çŸ¥çš„æ¤œç´¢ãƒ‡ãƒ¢
        await intelligent_search_demo(service)
        
        # è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªãƒ‡ãƒ¢
        await natural_language_query_demo(service)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        await performance_analysis_demo(service)
        
        # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
        await display_enhanced_stats(service)
        
    except Exception as e:
        logger.error(f"Enhanced demo failed: {e}")
    
    finally:
        if service:
            await service.shutdown()
            logger.info("ğŸ”Œ Enhanced document service shutdown completed")

async def display_enhanced_stats(service: EnhancedDocumentService):
    """æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆè¡¨ç¤º"""
    try:
        logger.info("\nğŸ“Š Enhanced System Statistics:")
        stats = await service.get_enhanced_system_stats()
        
        # åŸºæœ¬çµ±è¨ˆ
        if "mongodb" in stats and stats["mongodb"]:
            mongo_stats = stats["mongodb"]
            logger.info(f"  ğŸ“„ MongoDB: {mongo_stats.get('total_documents', 0)} documents")
        
        if "milvus" in stats and stats["milvus"]:
            milvus_stats = stats["milvus"]
            logger.info(f"  ğŸ” Milvus: Vector store ready - {milvus_stats.get('status', 'unknown')}")
        
        # Neo4jæ‹¡å¼µçµ±è¨ˆ
        if "neo4j_enhanced" in stats and stats["neo4j_enhanced"]:
            neo4j_stats = stats["neo4j_enhanced"]
            logger.info(f"  ğŸ•¸ï¸  Neo4j Enhanced:")
            logger.info(f"    - Documents: {neo4j_stats.get('total_documents', 0)}")
            logger.info(f"    - Tags: {neo4j_stats.get('total_tags', 0)}")
            logger.info(f"    - Relationships: {neo4j_stats.get('total_relationships', 0)}")
        
        # Ollamaæ€§èƒ½çµ±è¨ˆ
        if "ollama_performance" in stats and stats["ollama_performance"]:
            ollama_stats = stats["ollama_performance"]
            logger.info(f"  ğŸ¤– Ollama Performance:")
            logger.info(f"    - Embeddings generated: {ollama_stats.get('embeddings_generated', 0)}")
            logger.info(f"    - Cache hit ratio: {ollama_stats.get('cache_hit_ratio', 0):.2%}")
            logger.info(f"    - Cache size: {ollama_stats.get('cache_size', 0)}")
        
        # LlamaIndexçŠ¶æ…‹
        if "llamaindex_status" in stats and stats["llamaindex_status"]:
            li_stats = stats["llamaindex_status"]
            logger.info(f"  ğŸ¦™ LlamaIndex Status:")
            logger.info(f"    - Vector Index: {'âœ…' if li_stats.get('vector_index_ready') else 'âŒ'}")
            logger.info(f"    - Query Engine: {'âœ…' if li_stats.get('query_engine_ready') else 'âŒ'}")
            logger.info(f"    - Retriever: {'âœ…' if li_stats.get('retriever_ready') else 'âŒ'}")
        
        logger.info("")
        
    except Exception as e:
        logger.error(f"Failed to display enhanced stats: {e}")

async def create_enhanced_sample_documents(service: EnhancedDocumentService):
    """æ‹¡å¼µã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ"""
    try:
        logger.info("ğŸ“ Creating enhanced sample documents...")
        
        # ã‚ˆã‚Šè©³ç´°ãªã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
        enhanced_sample_files = [
            {
                "filename": "advanced_ai_research.txt",
                "content": """æœ€æ–°ã®äººå·¥çŸ¥èƒ½ç ”ç©¶å‹•å‘

è¿‘å¹´ã®äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ç ”ç©¶ã¯é©šç•°çš„ãªé€²æ­©ã‚’é‚ã’ã¦ã„ã¾ã™ã€‚ç‰¹ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®ç™ºå±•ã«ã‚ˆã‚Šã€è‡ªç„¶è¨€èªå‡¦ç†ã€æ©Ÿæ¢°ç¿»è¨³ã€æ–‡æ›¸ç”Ÿæˆã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãªã©ã®åˆ†é‡ã§é©æ–°çš„ãªæˆæœãŒç”Ÿã¾ã‚Œã¦ã„ã¾ã™ã€‚

ä¸»è¦ãªç ”ç©¶åˆ†é‡ï¼š

1. ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
Attentionæ©Ÿæ§‹ã‚’åŸºç›¤ã¨ã™ã‚‹Transformerã¯ã€BERTã€GPTã€T5ãªã©ã®ãƒ¢ãƒ‡ãƒ«ã®åŸºç¤ã¨ãªã£ã¦ã„ã¾ã™ã€‚è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã«ã‚ˆã‚Šã€é•·è·é›¢ã®ä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã§ãã¾ã™ã€‚

2. ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AI
CLIPã€DALL-Eã€GPT-4ã®ã‚ˆã†ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã‚’çµ±åˆçš„ã«å‡¦ç†ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIãŒæ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šäººé–“ã«è¿‘ã„ç†è§£ã¨ç”ŸæˆãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

3. å¼·åŒ–å­¦ç¿’ã¨ã®èåˆ
ChatGPTã§ç”¨ã„ã‚‰ã‚ŒãŸRLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰ã®ã‚ˆã†ã«ã€å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦AIã®å‡ºåŠ›ã‚’äººé–“ã®ä¾¡å€¤è¦³ã«åˆã‚ã›ã‚‹ç ”ç©¶ãŒé€²ã‚“ã§ã„ã¾ã™ã€‚

4. åŠ¹ç‡åŒ–æŠ€è¡“
å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã€è’¸ç•™ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã€é‡å­åŒ–ã€LoRAãªã©ã®åŠ¹ç‡åŒ–æŠ€è¡“ãŒé–‹ç™ºã•ã‚Œã¦ã„ã¾ã™ã€‚

ä»Šå¾Œã®å±•æœ›ï¼š
- AGIï¼ˆArtificial General Intelligenceï¼‰ã«å‘ã‘ãŸç ”ç©¶
- èª¬æ˜å¯èƒ½AIï¼ˆXAIï¼‰ã®ç™ºå±•
- ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®AIå®Ÿè¡Œ
- AIå€«ç†ã¨ã‚¬ãƒãƒŠãƒ³ã‚¹ã®ç¢ºç«‹

ã“ã‚Œã‚‰ã®æŠ€è¡“ã¯ã€åŒ»ç™‚ã€æ•™è‚²ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã€è£½é€ æ¥­ãªã©ã€ã‚ã‚‰ã‚†ã‚‹åˆ†é‡ã§ã®å¿œç”¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚""",
                "metadata": {
                    "category": "research", 
                    "topic": "artificial_intelligence",
                    "research_level": "advanced",
                    "publication_year": 2024
                }
            },
            {
                "filename": "quantum_computing_basics.txt",
                "content": """é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å…¥é–€

é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯ã€é‡å­åŠ›å­¦ã®åŸç†ã‚’åˆ©ç”¨ã—ãŸé©æ–°çš„ãªè¨ˆç®—æŠ€è¡“ã§ã™ã€‚å¾“æ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¨ã¯æ ¹æœ¬çš„ã«ç•°ãªã‚‹åŸç†ã§å‹•ä½œã—ã€ç‰¹å®šã®å•é¡Œã«å¯¾ã—ã¦æŒ‡æ•°çš„ãªé«˜é€ŸåŒ–ã‚’å®Ÿç¾ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

åŸºæœ¬æ¦‚å¿µï¼š

1. é‡å­ãƒ“ãƒƒãƒˆï¼ˆQubitï¼‰
å¾“æ¥ã®ãƒ“ãƒƒãƒˆã¯0ã¾ãŸã¯1ã®çŠ¶æ…‹ã‚’ã¨ã‚Šã¾ã™ãŒã€é‡å­ãƒ“ãƒƒãƒˆã¯0ã¨1ã®é‡ã­åˆã‚ã›çŠ¶æ…‹ã‚’æŒã¦ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€nå€‹ã®é‡å­ãƒ“ãƒƒãƒˆã§2^nå€‹ã®çŠ¶æ…‹ã‚’åŒæ™‚ã«è¡¨ç¾ã§ãã¾ã™ã€‚

2. é‡å­ã‚‚ã¤ã‚Œï¼ˆEntanglementï¼‰
è¤‡æ•°ã®é‡å­ãƒ“ãƒƒãƒˆé–“ã«å¼·ã„ç›¸é–¢é–¢ä¿‚ãŒç”Ÿã¾ã‚Œã‚‹ç¾è±¡ã§ã™ã€‚ä¸€æ–¹ã®é‡å­ãƒ“ãƒƒãƒˆã®çŠ¶æ…‹ã‚’æ¸¬å®šã™ã‚‹ã¨ã€ç¬æ™‚ã«ä»–æ–¹ã®çŠ¶æ…‹ãŒæ±ºã¾ã‚Šã¾ã™ã€‚

3. é‡å­å¹²æ¸‰ï¼ˆInterferenceï¼‰
é‡å­çŠ¶æ…‹ã®æŒ¯å¹…ãŒå¹²æ¸‰ã—åˆã†ã“ã¨ã§ã€æ­£ã—ã„ç­”ãˆã®ç¢ºç‡ã‚’é«˜ã‚ã€é–“é•ã£ãŸç­”ãˆã®ç¢ºç‡ã‚’ä½ãã—ã¾ã™ã€‚

ä¸»è¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼š
- Shorã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : ç´ å› æ•°åˆ†è§£ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œ
- Groverã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’é«˜é€ŸåŒ–
- VQEï¼ˆVariational Quantum Eigensolverï¼‰: åˆ†å­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

å¿œç”¨åˆ†é‡ï¼š
- æš—å·è§£èª­ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- è–¬ç‰©ç™ºè¦‹ã¨åˆ†å­ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- æœ€é©åŒ–å•é¡Œ
- æ©Ÿæ¢°å­¦ç¿’ã®é«˜é€ŸåŒ–

ç¾åœ¨ã®èª²é¡Œï¼š
- é‡å­ãƒ‡ã‚³ãƒ’ãƒ¼ãƒ¬ãƒ³ã‚¹ï¼ˆãƒã‚¤ã‚ºï¼‰
- ã‚¨ãƒ©ãƒ¼ä¿®æ­£æŠ€è¡“
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
- é‡å­ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™º

IBMã€Googleã€Microsoftã€Amazonãªã©ã®ä¼æ¥­ãŒé‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®å®Ÿç”¨åŒ–ã«å‘ã‘ã¦ç«¶äº‰ã—ã¦ãŠã‚Šã€NISQï¼ˆNoisy Intermediate-Scale Quantumï¼‰æ™‚ä»£ã‹ã‚‰æœ¬æ ¼çš„ãªé‡å­å„ªä½æ€§ã®å®Ÿç¾ã¸ã¨é€²ã‚“ã§ã„ã¾ã™ã€‚""",
                "metadata": {
                    "category": "technology", 
                    "topic": "quantum_computing",
                    "difficulty": "intermediate",
                    "target_audience": "engineers"
                }
            },
            {
                "filename": "sustainable_technology.txt",
                "content": """æŒç¶šå¯èƒ½ãªæŠ€è¡“é©æ–°

æ°—å€™å¤‰å‹•ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼å±æ©Ÿã¸ã®å¯¾å¿œã¨ã—ã¦ã€æŒç¶šå¯èƒ½ãªæŠ€è¡“é©æ–°ãŒæ€¥å‹™ã¨ãªã£ã¦ã„ã¾ã™ã€‚ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼æ¥­ç•Œã‚‚ç’°å¢ƒè² è·ã‚’å‰Šæ¸›ã—ã€å¾ªç’°å‹ç¤¾ä¼šã®å®Ÿç¾ã«å‘ã‘ãŸå–ã‚Šçµ„ã¿ã‚’åŠ é€Ÿã•ã›ã¦ã„ã¾ã™ã€‚

ä¸»è¦ãªæŠ€è¡“åˆ†é‡ï¼š

1. å†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼æŠ€è¡“
- å¤ªé™½å…‰ç™ºé›»ã®åŠ¹ç‡å‘ä¸Šï¼ˆãƒšãƒ­ãƒ–ã‚¹ã‚«ã‚¤ãƒˆå¤ªé™½é›»æ± ã€ã‚¿ãƒ³ãƒ‡ãƒ å‹ã‚»ãƒ«ï¼‰
- é¢¨åŠ›ç™ºé›»ã®å¤§å‹åŒ–ã¨æ´‹ä¸Šå±•é–‹
- åœ°ç†±ã€æ½®åŠ›ã€ãƒã‚¤ã‚ªãƒã‚¹ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®æ´»ç”¨
- ã‚¨ãƒãƒ«ã‚®ãƒ¼è²¯è”µæŠ€è¡“ï¼ˆãƒªãƒã‚¦ãƒ ã‚¤ã‚ªãƒ³é›»æ± ã€å›ºä½“é›»æ± ã€æ°´ç´ ç‡ƒæ–™é›»æ± ï¼‰

2. ã‚°ãƒªãƒ¼ãƒ³ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- çœé›»åŠ›ãƒ—ãƒ­ã‚»ãƒƒã‚µã®é–‹ç™º
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã®å†·å´åŠ¹ç‡åŒ–
- ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã®æœ€é©åŒ–
- AIã‚’æ´»ç”¨ã—ãŸã‚¨ãƒãƒ«ã‚®ãƒ¼ç®¡ç†

3. ã‚µãƒ¼ã‚­ãƒ¥ãƒ©ãƒ¼ã‚¨ã‚³ãƒãƒŸãƒ¼
- ãƒªã‚µã‚¤ã‚¯ãƒ«æŠ€è¡“ã®å‘ä¸Š
- ãƒã‚¤ã‚ªãƒ™ãƒ¼ã‚¹ææ–™ã®é–‹ç™º
- è£½å“ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†
- ã‚·ã‚§ã‚¢ãƒªãƒ³ã‚°ã‚¨ã‚³ãƒãƒŸãƒ¼ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

4. ã‚«ãƒ¼ãƒœãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«æŠ€è¡“
- ç‚­ç´ å›åãƒ»åˆ©ç”¨ãƒ»è²¯ç•™ï¼ˆCCUSï¼‰
- ç›´æ¥ç©ºæ°—å›åï¼ˆDACï¼‰
- ãƒ¡ã‚¿ãƒãƒ¼ã‚·ãƒ§ãƒ³æŠ€è¡“
- ã‚°ãƒªãƒ¼ãƒ³æ°´ç´ è£½é€ 

ãƒ‡ã‚¸ã‚¿ãƒ«æŠ€è¡“ã®æ´»ç”¨ï¼š
- IoTã«ã‚ˆã‚‹ç’°å¢ƒãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
- AIã‚’æ´»ç”¨ã—ãŸæœ€é©åŒ–
- ãƒ–ãƒ­ãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ã«ã‚ˆã‚‹é€æ˜æ€§ç¢ºä¿
- ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ã«ã‚ˆã‚‹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ä¼æ¥­ã®å–ã‚Šçµ„ã¿ï¼š
- RE100ï¼ˆå†ç”Ÿå¯èƒ½ã‚¨ãƒãƒ«ã‚®ãƒ¼100%ï¼‰ã¸ã®å‚åŠ 
- ã‚«ãƒ¼ãƒœãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«å®£è¨€
- ESGæŠ•è³‡ã®æ‹¡å¤§
- ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ã®é€æ˜åŒ–

æ”¿ç­–çš„æ”¯æ´ï¼š
- ã‚°ãƒªãƒ¼ãƒ³ãƒ‹ãƒ¥ãƒ¼ãƒ‡ã‚£ãƒ¼ãƒ«æ”¿ç­–
- ã‚«ãƒ¼ãƒœãƒ³ãƒ—ãƒ©ã‚¤ã‚·ãƒ³ã‚°
- æŠ€è¡“é–‹ç™ºã¸ã®è£œåŠ©é‡‘
- å›½éš›å”åŠ›ã®æ¨é€²

æŒç¶šå¯èƒ½ãªæŠ€è¡“é©æ–°ã¯ã€ç’°å¢ƒä¿è­·ã¨çµŒæ¸ˆæˆé•·ã®ä¸¡ç«‹ã‚’å¯èƒ½ã«ã—ã€æ¬¡ä¸–ä»£ã¸ã®è²¬ä»»ã‚’æœãŸã™éµã¨ãªã‚Šã¾ã™ã€‚æŠ€è¡“è€…ã€ä¼æ¥­ã€æ”¿åºœãŒé€£æºã—ã¦ã€ã‚ˆã‚Šè‰¯ã„æœªæ¥ã‚’ç¯‰ã„ã¦ã„ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚""",
                "metadata": {
                    "category": "sustainability", 
                    "topic": "green_technology",
                    "impact": "global",
                    "urgency": "high"
                }
            }
        ]
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã¨æ‹¡å¼µå‡¦ç†
        for file_info in enhanced_sample_files:
            file_path = project_root / file_info["filename"]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(file_info["content"])
            
            # æ‹¡å¼µãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†
            document_id = await service.process_file_enhanced(
                str(file_path), 
                file_info["metadata"],
                extract_entities=True,
                analyze_sentiment=True
            )
            
            if document_id:
                logger.info(f"  âœ… Enhanced: {file_info['filename']} (ID: {document_id[:8]}...)")
            else:
                logger.error(f"  âŒ Failed: {file_info['filename']}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
            if file_path.exists():
                file_path.unlink()
        
        logger.info("ğŸ“ Enhanced sample documents created successfully\n")
        
    except Exception as e:
        logger.error(f"Failed to create enhanced sample documents: {e}")

async def intelligent_search_demo(service: EnhancedDocumentService):
    """çŸ¥çš„æ¤œç´¢ãƒ‡ãƒ¢"""
    try:
        logger.info("ğŸ§  Intelligent Search Demo:")
        
        enhanced_queries = [
            "é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã®åŸºæœ¬åŸç†ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ",
            "AIç ”ç©¶ã«ãŠã‘ã‚‹æœ€æ–°ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼æŠ€è¡“",
            "æŒç¶šå¯èƒ½ãªã‚¨ãƒãƒ«ã‚®ãƒ¼æŠ€è¡“ã®ç™ºå±•",
            "æ©Ÿæ¢°å­¦ç¿’ã¨å¼·åŒ–å­¦ç¿’ã®èåˆ",
            "ç’°å¢ƒæŠ€è¡“ã¨ãƒ‡ã‚¸ã‚¿ãƒ«é©æ–°"
        ]
        
        for query in enhanced_queries:
            logger.info(f"\n  ğŸ” Intelligent Query: '{query}'")
            
            # çŸ¥çš„æ¤œç´¢å®Ÿè¡Œ
            results = await service.intelligent_search(
                query, 
                search_type="hybrid", 
                include_metadata=True,
                rerank=True,
                limit=3
            )
            
            if results:
                for i, result in enumerate(results, 1):
                    logger.info(f"    {i}. {result.document_title} (Score: {result.score:.3f})")
                    logger.info(f"       {result.content[:120]}...")
                    if result.metadata.get("entities"):
                        entities = result.metadata["entities"][:2]  # æœ€åˆã®2ã¤ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
                        entity_names = [e.get("name", "") for e in entities]
                        logger.info(f"       Entities: {', '.join(entity_names)}")
            else:
                logger.info("    No results found")
        
        logger.info("\nğŸ§  Intelligent search demo completed\n")
        
    except Exception as e:
        logger.error(f"Intelligent search demo failed: {e}")

async def natural_language_query_demo(service: EnhancedDocumentService):
    """è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªãƒ‡ãƒ¢"""
    try:
        logger.info("ğŸ’¬ Natural Language Query Demo:")
        
        nl_questions = [
            "AIã¨é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é–¢ä¿‚ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            "æŒç¶šå¯èƒ½ãªæŠ€è¡“ã«ã¯ã©ã®ã‚ˆã†ãªç¨®é¡ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ",
            "æ©Ÿæ¢°å­¦ç¿’ã®æœ€æ–°ãƒˆãƒ¬ãƒ³ãƒ‰ã¯ä½•ã§ã™ã‹ï¼Ÿ"
        ]
        
        for question in nl_questions:
            logger.info(f"\n  â“ Question: '{question}'")
            
            # è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            response = await service.natural_language_query(question)
            
            logger.info(f"  ğŸ’¡ Answer: {response.get('answer', 'No answer available')[:200]}...")
            logger.info(f"  ğŸ“Š Confidence: {response.get('confidence', 0.0):.2%}")
            logger.info(f"  ğŸ“š Sources: {len(response.get('sources', []))} documents")
        
        logger.info("\nğŸ’¬ Natural language query demo completed\n")
        
    except Exception as e:
        logger.error(f"Natural language query demo failed: {e}")

async def performance_analysis_demo(service: EnhancedDocumentService):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ‡ãƒ¢"""
    try:
        logger.info("âš¡ Performance Analysis Demo:")
        
        # Ollamaçµ±è¨ˆå–å¾—
        from ollama_client import ollama_client
        perf_stats = await ollama_client.get_performance_stats()
        
        logger.info(f"  ğŸ¤– Ollama Performance:")
        logger.info(f"    - Total embeddings: {perf_stats.get('embeddings_generated', 0)}")
        logger.info(f"    - Total text generations: {perf_stats.get('text_generated', 0)}")
        logger.info(f"    - Cache hits: {perf_stats.get('cache_hits', 0)}")
        logger.info(f"    - Cache misses: {perf_stats.get('cache_misses', 0)}")
        logger.info(f"    - Hit ratio: {perf_stats.get('cache_hit_ratio', 0):.2%}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ãƒ†ã‚¹ãƒˆ
        logger.info("\n  ğŸ§¹ Testing cache clear...")
        await ollama_client.clear_cache()
        logger.info("  âœ… Cache cleared successfully")
        
        logger.info("\nâš¡ Performance analysis completed\n")
        
    except Exception as e:
        logger.error(f"Performance analysis failed: {e}")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ Enhanced demo interrupted by user")
    except Exception as e:
        logger.error(f"Enhanced demo failed: {e}")
        sys.exit(1)