import asyncio
import logging
import time
from typing import List, Dict, Any
import json
from pathlib import Path

from enhanced_document_service import EnhancedDocumentService

logger = logging.getLogger(__name__)

class PromptSearchBenchmark:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¤œç´¢æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    
    def __init__(self, service: EnhancedDocumentService):
        self.service = service
        self.results = []
    
    async def run_benchmark(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting prompt search benchmark...")
        
        total_start_time = time.time()
        
        for i, test_case in enumerate(test_cases):
            logger.info(f"ğŸ“ Test case {i+1}/{len(test_cases)}: {test_case['name']}")
            
            # å„æ¤œç´¢æ‰‹æ³•ã‚’ãƒ†ã‚¹ãƒˆ
            case_results = await self._run_single_test_case(test_case)
            self.results.append(case_results)
        
        total_time = time.time() - total_start_time
        
        # çµ±è¨ˆè¨ˆç®—
        benchmark_stats = self._calculate_benchmark_stats(total_time)
        
        logger.info("ğŸ Benchmark completed!")
        return benchmark_stats
    
    async def _run_single_test_case(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®Ÿè¡Œ"""
        prompt = test_case['prompt']
        expected_topics = test_case.get('expected_topics', [])
        
        case_result = {
            'name': test_case['name'],
            'prompt': prompt,
            'expected_topics': expected_topics,
            'methods': {}
        }
        
        # 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¤œç´¢
        start_time = time.time()
        prompt_results = await self.service.search_by_prompt(
            prompt,
            similarity_threshold=0.6,
            max_results=5
        )
        prompt_time = time.time() - start_time
        
        case_result['methods']['prompt_search'] = {
            'execution_time': prompt_time,
            'result_count': len(prompt_results),
            'results': prompt_results[:3],  # ä¸Šä½3ä»¶ã®ã¿ä¿å­˜
            'avg_score': sum([r.get('similarity_score', 0) for r in prompt_results]) / max(1, len(prompt_results))
        }
        
        # 2. çŸ¥çš„æ¤œç´¢
        start_time = time.time()
        intelligent_results = await self.service.intelligent_search(
            prompt,
            search_type="hybrid",
            limit=5
        )
        intelligent_time = time.time() - start_time
        
        case_result['methods']['intelligent_search'] = {
            'execution_time': intelligent_time,
            'result_count': len(intelligent_results),
            'results': [{'title': r.document_title, 'score': r.score} for r in intelligent_results[:3]],
            'avg_score': sum([r.score for r in intelligent_results]) / max(1, len(intelligent_results))
        }
        
        # 3. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿
        start_time = time.time()
        vector_results = await self.service.intelligent_search(
            prompt,
            search_type="vector",
            limit=5
        )
        vector_time = time.time() - start_time
        
        case_result['methods']['vector_search'] = {
            'execution_time': vector_time,
            'result_count': len(vector_results),
            'results': [{'title': r.document_title, 'score': r.score} for r in vector_results[:3]],
            'avg_score': sum([r.score for r in vector_results]) / max(1, len(vector_results))
        }
        
        # é–¢é€£æ€§è©•ä¾¡
        case_result['relevance_evaluation'] = self._evaluate_relevance(
            prompt_results, expected_topics
        )
        
        return case_result
    
    def _evaluate_relevance(self, results: List[Dict], expected_topics: List[str]) -> Dict[str, Any]:
        """é–¢é€£æ€§è©•ä¾¡"""
        if not expected_topics:
            return {'score': 0.0, 'explanation': 'No expected topics provided'}
        
        relevant_count = 0
        total_results = len(results)
        
        for result in results:
            tags = result.get('tags', [])
            title = result.get('title', '').lower()
            
            # ã‚¿ã‚°ã‚„ ã‚¿ã‚¤ãƒˆãƒ«ã«æœŸå¾…ãƒˆãƒ”ãƒƒã‚¯ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for topic in expected_topics:
                if (topic.lower() in title or 
                    any(topic.lower() in tag.lower() for tag in tags)):
                    relevant_count += 1
                    break
        
        relevance_score = relevant_count / max(1, total_results)
        
        return {
            'score': relevance_score,
            'relevant_count': relevant_count,
            'total_count': total_results,
            'explanation': f'{relevant_count}/{total_results} results matched expected topics'
        }
    
    def _calculate_benchmark_stats(self, total_time: float) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµ±è¨ˆè¨ˆç®—"""
        stats = {
            'total_execution_time': total_time,
            'total_test_cases': len(self.results),
            'method_performance': {},
            'overall_metrics': {}
        }
        
        # å„æ‰‹æ³•ã®çµ±è¨ˆ
        methods = ['prompt_search', 'intelligent_search', 'vector_search']
        
        for method in methods:
            method_times = []
            method_scores = []
            method_counts = []
            
            for result in self.results:
                if method in result['methods']:
                    method_data = result['methods'][method]
                    method_times.append(method_data['execution_time'])
                    method_scores.append(method_data['avg_score'])
                    method_counts.append(method_data['result_count'])
            
            if method_times:
                stats['method_performance'][method] = {
                    'avg_execution_time': sum(method_times) / len(method_times),
                    'min_execution_time': min(method_times),
                    'max_execution_time': max(method_times),
                    'avg_score': sum(method_scores) / len(method_scores),
                    'avg_result_count': sum(method_counts) / len(method_counts)
                }
        
        # é–¢é€£æ€§çµ±è¨ˆ
        relevance_scores = [r['relevance_evaluation']['score'] for r in self.results]
        if relevance_scores:
            stats['overall_metrics']['avg_relevance'] = sum(relevance_scores) / len(relevance_scores)
            stats['overall_metrics']['min_relevance'] = min(relevance_scores)
            stats['overall_metrics']['max_relevance'] = max(relevance_scores)
        
        return stats
    
    def save_results(self, filename: str):
        """çµæœä¿å­˜"""
        output_data = {
            'benchmark_results': self.results,
            'timestamp': time.time(),
            'test_count': len(self.results)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“Š Benchmark results saved to {filename}")

async def main():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
    test_cases = [
        {
            'name': 'Python Programming Query',
            'prompt': 'Pythonã§ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å§‹ã‚ãŸã„åˆå¿ƒè€…å‘ã‘ã®æƒ…å ±',
            'expected_topics': ['python', 'programming', 'data']
        },
        {
            'name': 'Machine Learning Algorithm Selection',
            'prompt': 'åˆ†é¡å•é¡Œã«æœ€é©ãªæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é¸ã³ãŸã„',
            'expected_topics': ['machine_learning', 'algorithms', 'classification']
        },
        {
            'name': 'Web Development Framework',
            'prompt': 'ãƒ¢ãƒ€ãƒ³ãªã‚¦ã‚§ãƒ–ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯',
            'expected_topics': ['web', 'development', 'framework']
        },
        {
            'name': 'Mobile App Development',
            'prompt': 'ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªé–‹ç™ºæ‰‹æ³•',
            'expected_topics': ['mobile', 'development', 'cross_platform']
        },
        {
            'name': 'Data Visualization Tools',
            'prompt': 'ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã«ä½¿ãˆã‚‹ãƒ„ãƒ¼ãƒ«ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª',
            'expected_topics': ['data', 'visualization', 'tools']
        },
        {
            'name': 'AI Research Trends',
            'prompt': 'äººå·¥çŸ¥èƒ½ç ”ç©¶ã®æœ€æ–°å‹•å‘ã¨æŠ€è¡“',
            'expected_topics': ['ai', 'research', 'technology']
        },
        {
            'name': 'Database Technology',
            'prompt': 'ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆæ‰‹æ³•',
            'expected_topics': ['database', 'scalability', 'design']
        },
        {
            'name': 'Cloud Computing Services',
            'prompt': 'ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã®é¸æŠã¨æ¯”è¼ƒ',
            'expected_topics': ['cloud', 'services', 'comparison']
        }
    ]
    
    service = None
    
    try:
        logger.info("=== Prompt Search Benchmark ===")
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        service = EnhancedDocumentService()
        success = await service.initialize()
        
        if not success:
            logger.error("Failed to initialize service")
            return
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        benchmark = PromptSearchBenchmark(service)
        stats = await benchmark.run_benchmark(test_cases)
        
        # çµæœè¡¨ç¤º
        logger.info("\nğŸ“Š Benchmark Results Summary:")
        logger.info(f"Total execution time: {stats['total_execution_time']:.2f} seconds")
        logger.info(f"Total test cases: {stats['total_test_cases']}")
        
        if 'method_performance' in stats:
            logger.info("\nâš¡ Performance by Method:")
            for method, perf in stats['method_performance'].items():
                logger.info(f"  {method}:")
                logger.info(f"    Average time: {perf['avg_execution_time']:.3f}s")
                logger.info(f"    Average score: {perf['avg_score']:.3f}")
                logger.info(f"    Average results: {perf['avg_result_count']:.1f}")
        
        if 'overall_metrics' in stats:
            logger.info(f"\nğŸ¯ Overall Relevance: {stats['overall_metrics'].get('avg_relevance', 0):.2%}")
        
        # çµæœä¿å­˜
        benchmark.save_results('prompt_search_benchmark_results.json')
        
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
    
    finally:
        if service:
            await service.shutdown()

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())