version: '3.8'

services:
  ollama-base:
    build:
      context: .
      dockerfile: Dockerfile.base
    image: ollama-base:latest
    container_name: ollama-base-dev
    ports:
      - "${DEFAULT_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=${DEFAULT_HOST:-0.0.0.0}
    deploy:
      resources:
        limits:
          memory: ${MEMORY_LIMIT:-8g}
        reservations:
          memory: 2g
    networks:
      - ollama-network

  # 開発用サービス例
  ollama-llama2:
    build:
      context: .
      dockerfile: Dockerfile.model
      args:
        MODEL_NAME: llama2
    image: ollama-llama2:latest
    container_name: ollama-llama2-dev
    ports:
      - "11434:11434"
    volumes:
      - ollama_llama2_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - MODEL_NAME=llama2
    depends_on:
      - ollama-base
    networks:
      - ollama-network

volumes:
  ollama_data:
    driver: local
  ollama_llama2_data:
    driver: local

networks:
  ollama-network:
    driver: bridge